{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning of Tennis Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook determines the best set of hyperparameters to solve the Tennis enviornment as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set local parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.m5.large'\n",
    "n_timesteps = 3000\n",
    "n_instances = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the hyperparameters to optimize\n",
    "The following hyperparameters are read from the command line in [train.py](container/src/train.py).\n",
    "\n",
    "| Name        | Type  | Default | Description                        |\n",
    "|-------------|-------|---------|------------------------------------|\n",
    "| epochs      | int   |    2000 | number of total epochs to run      |\n",
    "| max_t       | int   |    1000 | max number of time steps per epoch |\n",
    "| fc1         | int   |     128 | size of 1st hidden layer           |\n",
    "| fc2         | int   |      64 | size of 2bd hidden layer           |\n",
    "| lr_actor    | float |   0.001 | initial learning rate for actor    |\n",
    "| lr_critic   | float |   0.001 | initial learning rate for critic   |\n",
    "| batch_size  | int   |     256 | mini batch size                    |\n",
    "| buffer_size | int   |  100000 | replay buffer size                 |\n",
    "| gamma       | float |     0.9 | discount factor                    |\n",
    "| tau         | float |   0.001 | soft update of target parameters   |\n",
    "| sigma       | float |    0.01 | OU Noise standard deviation        |\n",
    "\n",
    "Any of these could be tuned but we will down select to limit the search.    \n",
    "\n",
    "The hyperparameter tunner allow the hyperparameters to be defined as one of the following types. \n",
    "- `CategoricalParameter(list)` Categorical parameters need to take one value from a discrete set. \n",
    "- `ContinuousParameter(min, max)` Continuous parameters can take any real number value between the minimum and maximum value.\n",
    "- `IntegerParameter(min, max)` Integer parameters can take any integer value between the minimum and maximum value.\n",
    "\n",
    "_Note, if possible, it's almost always best to specify a value as the least restrictive type. For example, tuning learning rate as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with values 0.01, 0.1, 0.15, or 0.2. Some parameters are categorical to maintain a power 2 structure and limit the search space._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'encoder_num_hidden': IntegerParameter(1, 10),\n",
    "    'decoder_num_hidden': IntegerParameter(1, 10),\n",
    "    'batch_size': IntegerParameter(1, 256),\n",
    "    'learning_rate': ContinuousParameter(0.0001, 0.8),\n",
    "    'epochs': IntegerParameter(1, 1000),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the objective\n",
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. In this particular case, our script emits the total trainging episodes and we will use it as the objective metric, we also set the objective_type to be 'Minimize', so that hyperparameter tuning seeks to minize the objective metric when searching for the best hyperparameter setting. By default, objective_type is set to 'Maximize'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'time to solve'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': objective_metric_name,\n",
    "                       'Regex': '(\\S+) training objective.'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662572584943.dkr.ecr.us-east-1.amazonaws.com/rl-portfolio-optimization:latest\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "account = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "image_name = '{}.dkr.ecr.{}.amazonaws.com/rl-portfolio-optimization:latest'.format(account, region)\n",
    "print(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(role=role,\n",
    "                  instance_count=n_instances,\n",
    "                  instance_type=instance_type,\n",
    "                  image_uri=image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the hyperparameter tuner object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=5,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the hyperparameter tuning\n",
    "After the hyperprameter tuning job is created, you should be able to describe the tuning job to see its progress in the next step, and you can go to SageMaker console -> `Training` -> `Hyperparameter tuning jobs` to see the progresss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................................................................................................!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "tuner.fit()\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-09-26 23:51:51 Starting - Preparing the instances for training\n",
      "2021-09-26 23:51:51 Downloading - Downloading input data\n",
      "2021-09-26 23:51:51 Training - Training image download completed. Training in progress.\n",
      "2021-09-26 23:51:51 Uploading - Uploading generated training model\n",
      "2021-09-26 23:51:51 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "best_parameters = tuner.best_estimator().hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model = rl-portfolio-optimiz-210926-2347-004-43ddb435.\n",
      "\tencoder_num_hidden = 10\n",
      "\tdecoder_num_hidden = 1\n",
      "\tbatch_size = 256\n",
      "\tlearning_rate = 0.004326865191598946\n",
      "\tepochs = 1\n"
     ]
    }
   ],
   "source": [
    "best_name = tuner.best_training_job()\n",
    "print('\\nBest model = {}.'.format(best_name))\n",
    "for name in hyperparameter_ranges.keys():\n",
    "    print('\\t{} = {}'.format(name, best_parameters[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare optimal hyperparameters to manually optimized values.\n",
    "| Name        | Type  | Default | Description                        |\n",
    "|-------------|-------|---------|------------------------------------|\n",
    "| fc1         | int   |     128 | size of 1st hidden layer           |\n",
    "| fc2         | int   |      64 | size of 2bd hidden layer           |\n",
    "| lr_actor    | float |   0.001 | initial learning rate for actor    |\n",
    "| lr_critic   | float |   0.001 | initial learning rate for critic   |\n",
    "| batch_size  | int   |     256 | mini batch size                    |\n",
    "| buffer_size | int   |  100000 | replay buffer size                 |\n",
    "| gamma       | float |     0.9 | discount factor                    |\n",
    "| tau         | float |   0.001 | soft update of target parameters   |\n",
    "| sigma       | float |    0.01 | OU Noise standard deviation        |"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
